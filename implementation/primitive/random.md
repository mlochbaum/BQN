*View this file with results and syntax highlighting [here](https://mlochbaum.github.io/BQN/implementation/primitive/random.html).*

# Implementation of random stuff

BQN's random number utilities are provided by [system functions](../../spec/system.md#random-generation) and include some with non-obvious implementations. In the text below, `rand` represents any random number generator: `â€¢rand`, or a result of `â€¢MakeRand`.

## Random number generation

CBQN is currently using wyrand, part of the [wyhash](https://github.com/wangyi-fudan/wyhash) library. I don't recommend this and we'll probably switch away from it at some point, see below. Higher-quality (albeit slower) choices are [xoshiro++](https://prng.di.unimi.it/) and [PCG](https://www.pcg-random.org/). The authors of these algorithms (co-author for xoshiro) hate each other very much and have spent quite some time slinging mud at each other. As far as I can tell they both have the normal small bias in favor of their own algorithms but are wildly unfair towards the other side, choosing misleading examples and inflating minor issues. I think both generators are good but find the case for xoshiro a little more convincing, and I think it's done better in third-party benchmarks.

As for wyrand, it's based on an underlying 64-bit word of state that cycles around and is transformed to get each output, which is very fast but raises difficulties in getting reasonable statistics for consecutive outputs. If the state to output transformation was one-to-one, you'd never get the same output twice in a row. Of course this doesn't match the repetition chance of true random output; it turns out that a two-to-one mapping will get quite close in that statisticâ€”while failing in subtler ways, like having no chance of _three_ repetitions. But wyrand's output mapping looks like four-to-one based on measurements (and on the theoretical side there's one symmetry that obviously maps two-to-one, and some other suggestions of redundancy but no smoking gun for another). xoshiro author Sebastiano Vigna reported this issue [here](https://github.com/wangyi-fudan/wyhash/issues/130) and I replicated similar collision measurements in my own testing. In addition to the statistical issues, this means wyrand can only produce a fraction of the possible 64-bit output values, and there are [unrelated quality concerns](https://github.com/wangyi-fudan/wyhash/issues/135) as well.

## Shuffling

`rand.Deal` shuffles the natural numbers `â†•ğ•©`. To shuffle arbitrary values an extra selection is needed, which adds a little overhead for small values but can be more than double the cost for large ones (depending on available algorithms). So it may also make sense to have a `rand.Shuffle` defined as `{(ğ•¨ Deal â‰ ğ•©) âŠ ğ•©}`.

For small sizes the [Knuth shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle) works great. Initialize the whole result at the start; don't try anything fancy since it's only going to add branching. For large sizes CBQN uses one 256-way or smaller split as described below.
As it turns out, a method called [rip\_shuffle](https://crates.io/crates/rip_shuffle) based on similar principles, but with many additional complications to run parallel and in-place, was published just before I implemented that CBQN shuffle: see the paper [here](https://arxiv.org/abs/2302.03317).

Knuth shuffle makes random accesses by design, so if the result doesn't fit in cache there's going to be a serious slowdown. Sorting-based strategies mitigate this: the two methods I've found in the literature are Rao-Sandelius, like quicksort, and [MergeShuffle](https://arxiv.org/abs/1508.03167), like mergesort. A typical implementation of Rao-Sandelius sends each value to a random half in-place by swapping, and then the halves are shuffled. In fact, Sandelius described a decimal method with ten partitions, but his target hardware wasâ€¦ actual physical card decks, which don't have the same implementation issues present on CPUs. In MergeShuffle, halves are shuffled then merged, but the required merging technique is more sophisticated. However, I've found that the SIMD implementation presented by the authors is not actually too fast (once [corrected](https://github.com/axel-bacher/mergeshuffle/issues/1)) and branchlessly iterating over set bits of the random word works better.

MergeShuffle initially follows the naive strategy of choosing from one half or the other at random, with a single bit. On its own this is biased: the actual chance of selecting from a list should be proportional to the number of elements remaining in it. But at any given point the elements that _have_ been chosen are shuffled, assuming the initial lists were. This is true by symmetry because merge selections are made independently with the same probability, so given the number of selections from each side, any ordering is equally likely. Then MergeShuffle stops when it would try to get an element from an empty list, and shuffles the remaining elements into the result one at a time.

The same argument applies to multi-way merging, and more obliquely to splitting. Splitting is like radix sorting, which would typically require counting partition sizes. But it also works to make fixed-size partitions in the result, and stop when one runs out. The partitions have gaps between them but can be packed together by moving them as they're shuffled. Multi-way can be several times faster than binary because it does fewer movesâ€”binary needs one per bit.

Merging or splitting with two partitions is special in that it can be implemented in place with swapping; I don't think other numbers support this in general. But in `Deal` the first round of splitting has an implicit argument `â†•ğ•©`, so the split can be performed directly into result space with only an index and ending position for each partition. Merging or splitting each of these partitions would take a fraction of the space, but one 256-way split seems fine for CBQN as it only runs into cache trouble at sizes in the billions.

## Simple random sample

A [simple random sample](https://en.wikipedia.org/wiki/Simple_random_sample) from a set is a subset with a specified size, chosen so that each subset of that size has equal probability. `rand.Deal` gets a sample of size `ğ•¨` from the set `â†•ğ•©` with elements in a uniformly random order, and `rand.Subset` does the same but sorts the elements.

`Deal` uses a [Knuth shuffle](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle), stopping after the first `ğ•¨` elements have been shuffled, as the algorithm won't touch them again. Usually it creates `â†•ğ•©` explicitly for this purpose, but if `ğ•¨` is very small then initializing it is too slow. In this case we initialize `â†•ğ•¨`, but use a "hash" table with an identity hashâ€”the numbers are already randomâ€”for `ğ•¨â†“â†•ğ•©`. The default is that every value in the table is equal to its key, so that only entries where a swap has happened need to be stored. The hash table is the same design as for self-search functions, with open addressing and linear probing.

`Subset` uses [Floyd's method](https://math.stackexchange.com/questions/178690/whats-the-proof-of-correctness-for-robert-floyds-algorithm-for-selecting-a-sin), which is sort of a modification of shuffling where only the selected elements need to be stored, not what they were swapped with. This requires a lookup structure that can be updated efficiently and output all elements in sorted order. The choices are a bitset for large `ğ•¨` and another not-really-hash table for small `ğ•¨`. The table uses a right shiftâ€”that is, division by a power of twoâ€”as a hash so that hashing preserves the ordering, and inserts like an insertion sort: any larger entries are pushed forward. Really this is an online sorting algorithm, that works because we know the input distribution is well-behaved (it degrades to quadratic performance only in very unlikely cases). When `ğ•¨>ğ•©Ã·2`, we always use a bitset, but select `ğ•©-ğ•¨` elements and invert the selection.

I'm aware of [algorithms](https://richardstartin.github.io/posts/reservoir-sampling) like Vitter's Method D that generate a sorted sample in order, using the statistics of samples. There are a few reasons why I prefer Floyd's method. It's faster, because it uses one random generation per element while Vitter requires several, and it does less branching. It's exact, in that if it's given uniformly random numbers then it makes a uniformly random choice of sample. Vitter's method depends on floating-point randomness and irrational functions, so it can't accomplish this with finite precision random inputs. And the pattern of requests for Floyd's method is simple and deterministic. The advantage of reservoir algorithms like Vitter is that they are able to generate samples one at a time using a small fixed amount of memory. `â€¢MakeRand` only allows the user to request a sample all at once, so this advantage doesn't matter as much. The CBQN algorithms are tuned to use much more temporary memory than the size of the final result. It could be lowered, but there's usually plenty of temporary memory available.
