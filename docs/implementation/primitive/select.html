<head>
  <link href="../../favicon.ico" rel="shortcut icon" type="image/x-icon"/>
  <link href="../../style.css" rel="stylesheet"/>
  <meta charset="utf-8">
  <title>BQN: Implementation of Select</title>
</head>
<div class="nav">(<a href="https://github.com/mlochbaum/BQN">github</a>) / <a href="../../index.html">BQN</a> / <a href="../index.html">implementation</a> / <a href="index.html">primitive</a></div>
<h1 id="implementation-of-select"><a class="header" href="#implementation-of-select">Implementation of Select</a></h1>
<p><a href="../../doc/select.html">Select</a> is just a CPU load, right? Well, there is a bounds check first, but besides that, memory operations are slow. Yes, even if you use SIMD gathers‚Äîthese get rid of some minor instruction dispatching overhead but they're still executed as a bunch of separate memory requests. This means that, while there's no replacement for select in the general case, there are many sub-cases that can be handled faster in other ways.</p>
<p>The two important kinds of instructions for selection are the aforementioned <em>gather</em> instructions, which select multiple values from memory given a vector of indices, and <em>shuffle</em> instructions, which implement selection on a vector of indices and a vector of values. Shuffles don't touch memory and are many times faster.</p>
<h2 id="bounds-checking"><a class="header" href="#bounds-checking">Bounds checking</a></h2>
<p>Before doing any indexing you need to be sure that the indices are valid. And in a language like BQN that supports negative indices, they need to be wrapped around or otherwise accounted for. The obvious way to do this is to check before each load instruction, which is fairly expensive with scalar loads but much better with SIMD checking/wrapping and gathers.</p>
<p>An alternative approach is to do vectorized checking in blocks on <code><span class='Value'>ùï®</span></code> before selecting with that block. In this case, instead of simply testing whether all the indices fit, it's best to compute the minimum and maximum of each block. At one min and one max instruction per vector of indices this is probably faster than comparison in most cases, and it allows for additional optimizations:</p>
<ul>
<li>If the minimum is at least 0, there are no negative indices, so wrapping isn't needed. And if the maximum is less than 0, there are no <em>non</em>-negative indices, and a global offset can be used instead of wrapping.</li>
<li>If the minimum and maximum are equal, all indices are the same, so that cell of <code><span class='Value'>ùï©</span></code> can be copied without checking <code><span class='Value'>ùï®</span></code> again.</li>
<li>More generally, if the minimum and maximum are close, an appropriate small-range method can be used.</li>
</ul>
<p>After the range is found, another pass can wrap indices if needed by computing <code><span class='Value'>ùï®</span><span class='Function'>+</span><span class='Value'>n</span><span class='Function'>√ó</span><span class='Value'>ùï®</span><span class='Function'>&lt;</span><span class='Number'>0</span></code> with <code><span class='Value'>n</span><span class='Gets'>‚Üê</span><span class='Function'>‚â†</span><span class='Value'>ùï©</span></code>, which vectorizes easily. Unfortunately, the range after wrapping is unknown: 0 and -1 map to 0 and n-1 which spans the entire range, and aren't ruled out by any range that requires wrapping. Mixed-sign indices should be fairly rare, so it's probably fine to ignore small-range optimization in this case, but there could be another range check built into the wrapping code. When the range is small but crosses 0, it's also possible to copy the end of <code><span class='Value'>ùï©</span></code> and then the beginning, and select from this with an offset but no wrapping.</p>
<p>Extracting the max and min from an accumulator vector and dispatching to a special case has a non-negligible constant cost, so range checking does need to have a block size of many vector registers. This means the instructions mostly won't slip into time spent waiting on memory as they would with interleaved checks. The loss is less bad for smaller index types as SIMD range checking is faster, and I found that with generic code relying on auto-vectorization, separating the range check was better for 1-byte indices, about the same for 2-byte, and slower for larger indices. With any form of SIMD selection, incorporating a range check and wrap with selection will be faster because it makes better use of available throughput, so choosing to range-test first sacrifices raw speed for better special-case handling.</p>
<h2 id="small-range-selection"><a class="header" href="#small-range-selection">Small-range selection</a></h2>
<p>When <code><span class='Value'>ùï©</span></code> is small, or when values from <code><span class='Value'>ùï®</span></code> fit into a small range and thus select from a small slice of <code><span class='Value'>ùï©</span></code>, selection with vector shuffles can be much faster than memory-based selection. The relevant instructions in x86 are the SSSE3 shuffle and its AVX2 extension, which work on 1-byte values, and AVX2 vpermd (intrinsic permutevar8x32) on 4-byte values. NEON is cleaner and has vtbl instructions of various widths, but always works on 1-byte indices and values.</p>
<p>Vector shuffles are often effective even when the selection doesn't fit into a single register (or lane). Multiple shuffles can be combined with a blend or some bit-bashing emulation. Another trick applies to data wider than indices such as selecting 2-byte values with 1-byte indices: the indices could be expanded by zipping <code><span class='Number'>2</span><span class='Function'>√ó</span><span class='Value'>i</span></code> with <code><span class='Number'>1</span><span class='Function'>+</span><span class='Number'>2</span><span class='Function'>√ó</span><span class='Value'>i</span></code>, but a better method might be to <em>unzip</em> the values <code><span class='Value'>ùï©</span></code> before starting, and do the selection by shuffling both halves and them zipping those together.</p>
<p>The 1-byte shuffle can be used to do a 1-<em>bit</em> lookup from a table of up to 256 bits packed into vector registers. To look up a bit, select the appropriate byte from the table with the top 5 bits‚Äîif only 4 are used, a single shuffle instruction is enough, and for all 5 a blend of two shuffles is needed. Then use the bottom 3 bits to get a mask from another table where entry <code><span class='Value'>i</span></code> is <code><span class='Number'>1</span><span class='Function'>&lt;&lt;</span><span class='Value'>i</span></code>. And the mask with the byte to select the right bit, and pack into bits with a greater-than-0 comparison and movemask.</p>
<p>SIMD selection can be relevant to <a href="search.html#lookup-tables">lookup tables</a> for search functions, particularly bit selection for Member of.</p>
<h2 id="large-range-selection"><a class="header" href="#large-range-selection">Large-range selection</a></h2>
<p>When the selected elements don't fit in registers (and some other special case like sortedness doesn't apply‚Ä¶), you'll need a memory access per element. With AVX2 or ARM SVE there's a modest improvement for using gather instructions, except with Intel CPUs prior to Skylake (2015) where that's actually slower. If the supported types are wider than the ones you want to use, you can widen the indices, or select a larger element type and then narrow the elements.</p>
<p>For larger <code><span class='Value'>ùï©</span></code> arrays, random accesses can run out of cache space and become very expensive. As with most cache-incoherent operations, <a href="search.html#partitioning">radix partitioning</a> can be used to improve cache utilization. Partially sort the indices, perform selection, undo the sorting. However, the decision of whether to use it is much more difficult than for hash-based searches or shuffling, where accesses are random by design. Selection indices are quite likely to have some sort of locality that allows cache lines to be reused naturally. If this is the case, ordinary selection will be pretty fast, and partitioning is a lot of overhead to add to it. The only way I can think of to really test whether partitioning is needed is to sample some indices and find the number of unique cache lines represented. Figuring out how to choose the sample and interpret the statistics is tricky, and varying cache sizes and other effects will mean that you'll always get some edge cases wrong. But the downside of running a cache-incoherent selection naively is quite bad, so it's worth a try?</p>
<p>Booleans are messy. Probably gather, some vector shifts (or emulation with shuffle), and movemask to pack bits is the way to go if the instruction set supports it. If the selected array isn't too big relative to the number of selections, it can be expanded to 1-byte values as an easy way to avoid lots of bit manipulation.</p>
<h2 id="sorted-indices"><a class="header" href="#sorted-indices">Sorted indices</a></h2>
<p>When the indices <code><span class='Value'>ùï®</span></code> in <code><span class='Value'>ùï®</span><span class='Function'>‚äè</span><span class='Value'>ùï©</span></code> are <a href="flagsort.html">sorted</a>, slices from <code><span class='Value'>ùï®</span></code> often fit into a small range. A particular possibility of interest is <code><span class='Paren'>(</span><span class='Function'>+</span><span class='Modifier'>`</span><span class='Value'>bool</span><span class='Paren'>)</span><span class='Function'>‚äè</span><span class='Value'>ùï©</span></code>, where any slice of <code><span class='Value'>ùï®</span></code> selects from an equal or smaller (fewer elements) slice of <code><span class='Value'>ùï©</span></code>. Another useful pattern is <code><span class='Paren'>(</span><span class='Function'>‚åà</span><span class='Modifier'>`</span><span class='Function'>‚Üï</span><span class='Modifier2'>‚àò</span><span class='Function'>‚â†</span><span class='Modifier2'>‚ä∏</span><span class='Function'>√ó</span><span class='Value'>bool</span><span class='Paren'>)</span><span class='Function'>‚äè</span><span class='Value'>ùï©</span></code>, characterized by each result cell matching either the corresponding argument cell or previous result cell.</p>
<p>Taking any vector register out of <code><span class='Value'>ùï®</span></code>, several cases might apply:</p>
<ul>
<li>The indices are sparse, so the first one is far from the others. Scalar selection is needed.</li>
<li>Only the first few indices fall within a vector of <code><span class='Value'>ùï©</span></code>. An option is to use a shuffle for these indices, then start over with a vector beginning at the one after.</li>
<li>All indices fit into a vector of <code><span class='Value'>ùï©</span></code>. They can be handled with a shuffle, and possibly the <code><span class='Value'>ùï©</span></code> values could be retained for the next iteration.</li>
<li>All indices are the same. Copying the selected element of <code><span class='Value'>ùï©</span></code> is still a shuffle, but maybe the same vector works for more indices too? Hitting the speed of memset if there are many equal indices would be nice.</li>
<li>Indices are consecutive, always increasing by exactly 1. The corresponding elements of <code><span class='Value'>ùï©</span></code> can be copied as a unit.</li>
</ul>
<p>In the second case (partial shuffle), the number of handled indices can be found by comparing the index vector to the maximum allowed index (for example, first index plus 15), then using count-trailing-zeros on the resulting mask.</p>
<p>Choosing between these cases at every step would have high branching costs if the indices aren't very predictable. One way to keep branching costs down is to force each method to do some minimum amount of work if chosen. A hybrid of sparse and dense selection might search for places where dense selection applies by comparing a vector of indices, plus 15, to the vector shifted by 4. It can do sparse selection up to this point; when it hits the dense selection, getting to handle 4 indices quickly would outweigh the cost of a branch (if the number used is tuned properly and not something I made up).</p>
<p>A method with less branching is to take statistics in blocks. These might be used either as a hint, choosing between strategies that are fully general but adapted to different cases, or a proof, enabling a strategy that has some requirement. Some strategies that might be chosen this way are:</p>
<ul>
<li>Scalar selection.</li>
<li>Take a vector of indices, do as many as possible (it's at least 1), repeat until finished.</li>
<li>A hybrid dense/sparse method like the one just discussed.</li>
<li>Full vector selection, requiring indices 16 apart to differ by ‚â§16 in the 1-byte case.</li>
<li>memset, requiring indices to all be the same.</li>
<li>memcpy, requiring indices to be consecutive.</li>
</ul>
<p>So an example of a complex statistic is to test, for each index, whether the difference from the one before is at most 1 (so, the selection is equivalent to <code><span class='Paren'>(</span><span class='Function'>+</span><span class='Modifier'>`</span><span class='Value'>bool</span><span class='Paren'>)</span><span class='Function'>‚äè</span><span class='Value'>ùï©</span></code> for some <code><span class='Value'>bool</span></code>). If this holds, the full vector selection method‚Äîpick one vector from <code><span class='Value'>ùï®</span></code> and one from <code><span class='Value'>ùï©</span></code> beginning at the first index, apply a shuffle, write, move to the next vector‚Äîcan be used. If additionally the range is equal to the length, the differences all have to be 1, so that memcpy can be used.</p>
<p>The index range is a very easy statistic to compute, as it's just first minus last. In fact it's possible to pick the chunk size from a target range, using a binary search (which can stop at vector alignment or coarser since there's no need to find an exact element boundary, although if the range is 1 it's better to find it so the leftovers don't cause the range of the <em>next</em> section to be greater than 1). Since 1-byte indices are usually faster to work with, one approach might be to test if the next 64 indices (untested number for concreteness only) fit into a range of 256, handling them sparsely if not. If they are small-range then split into a few cases: range of 1, range 16, and range 256. In each case, keep increasing the chunk size by 64 as long as it stays in this range, or up to some maximum length chosen to avoid cache issues. Then in the range-1 case, copy that one element, in the range-16 case pick a single vector from <code><span class='Value'>ùï©</span></code> and shuffle it, and in the range-256 case copy the indices to a 1-byte buffer while taking statistics and choose what to do based on those. It's some sort of telescoping fractal around the sparse/dense boundary. The possibilities just keep going.</p>
<h2 id="odd-cell-sizes"><a class="header" href="#odd-cell-sizes">Odd cell sizes</a></h2>
<p>When <code><span class='Value'>ùï©</span></code> has rank greater than 1, <code><span class='Value'>ùï®</span><span class='Function'>‚äè</span><span class='Value'>ùï©</span></code> may select cells of any size, so one load instruction no longer does it. For long cells a general method like memcpy (or a bit-granularity version for booleans) is fine, so the concern is for short cells.</p>
<p>Overlapping in some way is fastest. One way is to round the cell size up, so that reading from <code><span class='Value'>ùï©</span></code> gets some additional elements and they'll be written to the result but overwritten by later iterations. Of course this requires writing result cells in order, and will also need some mechanism to make sure the last cell doesn't break anything. A possibly slower but tidier way is to have copy functions that carry over a specific number of bytes (regardless of underlying element type). For an odd size, let's say 11 bytes, it can copy the first 8 bytes and the last 8 bytes, overlapping internally. Since the same scheme works for any length from 9 to 16 bytes, there can be a single copy function that works on this range and takes an offset, keeping the total number of functions reasonable.</p>
<h3 id="odd-boolean-cells"><a class="header" href="#odd-boolean-cells">Odd boolean cells</a></h3>
<p>For booleans it's no longer possible to simply copy from one pointer to another. A bit offset needs to be specified, and, more importantly, writing needs to handle partial bytes, blending the written bits with some that are already there. Again there's the possibility of going in order so that only the beginning and not the end of the result cell needs to be handled; partial results could be kept in a buffer in order to write full words to the result. Currently in CBQN we have a slow generic method, but avoid using it if <code><span class='Value'>ùï©</span></code> is short enough by widening cells of <code><span class='Value'>ùï©</span></code> to a power of two or multiple of 64, selecting, and narrowing back to the original width (see <a href="take.html#bit-interleaving-and-uninterleaving">bit interleaving and uninterleaving</a>). This can't be used if <code><span class='Value'>ùï®</span></code> is very short relative to <code><span class='Value'>ùï©</span></code>, because it would do work on all of <code><span class='Value'>ùï©</span></code> just to select a few elements.</p>
<p>An alternative method for small boolean cells is to select directly from <code><span class='Value'>ùï©</span></code> but with a widened result, that is, over-read from each cell of <code><span class='Value'>ùï©</span></code>. Then at the end the result can be narrowed exactly as in the widen-select-narrow pattern. This goes into a temporary array, which can be over-allocated so there's no concern about the length of the last cell written. However, note that there are alignment concerns in reading from <code><span class='Value'>ùï©</span></code>: if the selection is done with 64-bit reads, it only works if every cell fits entirely in some word, which means that the cell width must be at most 60, and not equal to 59. Otherwise there will be some cells that span 9 bytes, and the selection will lose bits.</p>
<h2 id="select-cells"><a class="header" href="#select-cells">Select-cells</a></h2>
<p>Applying the same selection to each cell of an array, that is, <code><span class='Value'>inds</span><span class='Modifier2'>‚ä∏</span><span class='Function'>‚äè</span><span class='Modifier'>Àò</span> <span class='Value'>arr</span></code>, can be optimized similarly to small-range selection and is relevant to other array manipulation such as multidimensional Take, Drop, Replicate, and Rotate. For example <code><span class='Value'>r</span><span class='Modifier2'>‚ä∏</span><span class='Function'>/</span><span class='Modifier'>Àò</span> <span class='Value'>ùï©</span></code> may be best implemented as <code><span class='Paren'>(</span><span class='Function'>/</span><span class='Value'>r</span><span class='Paren'>)</span><span class='Modifier2'>‚ä∏</span><span class='Function'>‚äè</span><span class='Modifier'>Àò</span> <span class='Value'>ùï©</span></code>. It can also be applied to operations like <code><span class='Function'>‚çâ</span><span class='Modifier2'>‚éâ</span><span class='Number'>2</span></code> on multiple trailing axes, by treating them as a single axis and constructing indices appropriately.</p>
<p>Before getting to SIMD, one general way to reduce overhead if <code><span class='Value'>inds</span></code> is small is to convert to a single selection, <code><span class='Paren'>(</span><span class='Value'>inds</span><span class='Function'>+</span><span class='Modifier'>‚åúÀú</span><span class='Function'>‚Üï</span><span class='Modifier2'>‚ä∏</span><span class='Function'>√ó</span><span class='Modifier'>¬¥</span><span class='Function'>‚â¢</span><span class='Value'>arr</span><span class='Paren'>)</span><span class='Function'>‚äè‚•ä</span><span class='Value'>arr</span></code> for rank-2 <code><span class='Value'>arr</span></code> with adjustments possible for other ranks. This is effectively a specialization of the axis merging mentioned under <a href="#multi-axis-selection">multi-axis-selection</a>.</p>
<p>For vector selection, the basic idea is to optimize if each argument and result &quot;row&quot; fits in a vector, and apply shuffles to each row, over-reading argument and overlapping result rows as necessary. If multiple rows fit in a vector, the indices can be extended accordingly, adding another copy plus the cell size of <code><span class='Value'>ùï©</span></code>, another plus twice the cell size, and so on. The number of rows that can be handled at once might be constrained by either the argument row size or result row size (number of indices) as these two lengths can be independent. There's no obvious way to extend the selection size to a full vector, as (assuming argument and result cell sizes are the same for simplicity) a given aligned vector in the result may take values not just from the corresponding argument vector, but from adjacent ones as well. Also note that if the number of result values written at a time is less than half a vector, then an implementation that does partial writes at the end will need to be able to do more than one of them.</p>
<p>Multi-vector selection can also be used of course, increasing the argument row size that can be handled. Because <code><span class='Value'>ùï®</span></code> will be reused, it can be preprocessed to make this more effective. If the selection instruction returns 0 for some indices (x86 shuffles do this if the top bit is set), <code><span class='Value'>ùï®</span></code> can be split so that the selection step looks like the bitwise or of multiple selections, <code><span class='Value'>sel</span><span class='Paren'>(</span><span class='Value'>a</span><span class='Separator'>,</span><span class='Value'>x</span><span class='Paren'>)</span><span class='Function'>|</span><span class='Value'>sel</span><span class='Paren'>(</span><span class='Value'>b</span><span class='Separator'>,</span><span class='Value'>x</span><span class='Paren'>)</span><span class='Function'>|</span><span class='Value'>‚Ä¶</span></code>. There's an index vector for each index range <code><span class='Paren'>(</span><span class='Value'>k</span><span class='Function'>√ó</span><span class='Number'>16</span><span class='Paren'>)</span> <span class='Function'>‚â§</span> <span class='Value'>i</span> <span class='Function'>&lt;</span> <span class='Paren'>(</span><span class='Value'>k</span><span class='Function'>+</span><span class='Number'>1</span><span class='Paren'>)</span><span class='Function'>√ó</span><span class='Number'>16</span></code> which is set to <code><span class='Value'>i</span><span class='Function'>-</span><span class='Value'>k</span><span class='Function'>√ó</span><span class='Number'>16</span></code> for indices <code><span class='Value'>i</span></code> that fall in the range and 128 for other indices. If zeroing selection isn't available, other possibilities with blend or xor aren't too much worse. Increasing the result row size is much simpler, just do multiple selections on the same set of argument vectors. For <code><span class='Value'>x</span></code> input vectors and <code><span class='Value'>y</span></code> results you need <code><span class='Value'>x</span><span class='Function'>√ó</span><span class='Value'>y</span></code> index vectors, so vector selection quickly becomes impractical as sizes increase.</p>
<p>However, arbitrarily large result rows can be handled by constructing the result in columns, where each corresponds to a manageable slice of <code><span class='Value'>ùï®</span></code>. Working column-wise is effectively a way to avoid re-loading and initializing parts of <code><span class='Value'>ùï®</span></code>. It could also be applied to more complicated cases found by analyzing parts of <code><span class='Value'>ùï®</span></code>, such as the methods for <a href="#sorted-indices">sorted indices</a>. It's even possible it could be a slight improvement when used for scalar lookups, because only one load from <code><span class='Value'>ùï®</span></code> is needed per column.</p>
<p>The same pre-processing idea that works for multi-vector selection also speeds up selecting booleans. The masks from the bottom 3 bits, and shifted-down copy of the other bits, can be precomputed so that the selection step is to shuffle, bitwise-and with the masks, compare to 0, and pack to bit-boolean output.</p>
<h2 id="multi-axis-selection"><a class="header" href="#multi-axis-selection">Multi-axis selection</a></h2>
<p>To perform a selection where multiple axes are manipulated, trailing axes need to be handled with low per-cell overhead. Likely the best generic approach is to combine indices for lower axes arithmetically, by multiplying each by its stride in <code><span class='Value'>ùï©</span></code> and adding with <code><span class='Function'>+</span><span class='Modifier'>‚åú</span></code> (or some minor variation). This can be done starting from the bottom until the total size of the combined axes is large enough that time is spent mostly in selection itself instead of overhead around the selection loop (although I'm not sure how to handle the case where the last set of indices incorporated this way is huge‚Äîshould it be handled in sections somehow?). Then selection consists of some sort of multi-dimensional loop over leading axes, which doesn't need to be fast, with an inner selection using the list of indices and an offset pointer into <code><span class='Value'>ùï©</span></code>. This low/high axis division is similar to that suggested for <a href="transpose.html#arbitrary-reordering">multi-axis Transpose</a>.</p>
<p>If higher axes are expanded by the selection, so that there are more cells of some rank in the result than in <code><span class='Value'>ùï©</span></code>, it can be better to first apply the selection on the lower axes to give an intermediate array, and then do the selection on the upper axes, which will copy cells into the final result. Similarly, if higher axes have repeated indices, so that the unique cells of <code><span class='Value'>ùï©</span></code> used are smaller than the final result, a possible optimization is to split each repetitive leading index list <code><span class='Value'>i</span></code> into unique indices <code><span class='Function'>‚ç∑</span><span class='Value'>i</span></code> and where they appear in the result <code><span class='Function'>‚äê</span><span class='Value'>i</span></code>, with <code><span class='Function'>‚ç∑</span><span class='Value'>i</span></code> being used in the first step and <code><span class='Function'>‚äê</span><span class='Value'>i</span></code> in the second.</p>
<p>The base case with a list of indices might be replaced by any number of special cases after analyzing the lower indices:</p>
<ul>
<li>memcpy, for consecutive indices</li>
<li>offset+difference representation for arithmetic progressions</li>
<li>Take, for an unchanged axis above consecutive indices</li>
<li>any <a href="#select-cells">select-cells</a> method, for an unchanged axis above anything</li>
<li>Compress, for sorted unique indices</li>
</ul>
<p>An implementation like this could encompass Replicate, Select, Take, Drop, Rotate, and maybe Transpose! When faced with arguments specifying a complicated multi-dimensional operation, these primitives could be reduced to different entry points to a big selection engine. This engine could take an array and a list of transformations to apply to each axis. It could then put each transformation in a preferred form (analyzing indices and replicate arguments in particular), apply optimizations like merging unchanged axes, choose and initialize a bottom-level kernel, and apply it while looping over upper levels. Optimizations can be gated on the number of times an axis will be used, that is, the combined size of the result axes above it. For example, checking an index array to see if it's an arithmetic progression is unlikely to succeed, but if it's used a million times it's worth trying.</p>
